{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33916,
     "status": "ok",
     "timestamp": 1762243199997,
     "user": {
      "displayName": "ZeD",
      "userId": "14300756461738781152"
     },
     "user_tz": -330
    },
    "id": "W37QOm-24GoL",
    "outputId": "2bc9a4e5-271b-49cf-9b34-d430aec6d924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4406,
     "status": "ok",
     "timestamp": 1762244244185,
     "user": {
      "displayName": "ZeD",
      "userId": "14300756461738781152"
     },
     "user_tz": -330
    },
    "id": "11lovkNz2Pnp",
    "outputId": "994c253d-8073-4144-c5bb-e05bc7885970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Report for: /content/drive/My Drive/Colab Notebooks/adv.txt (Mode: text) ---\n",
      "\n",
      "Vocabulary Size: 16821\n",
      "\n",
      "10 Most Frequent Words:\n",
      "- the: 4795\n",
      "- and: 2572\n",
      "- of: 2458\n",
      "- to: 2435\n",
      "- i: 2420\n",
      "- a: 2344\n",
      "- in: 1607\n",
      "- that: 1494\n",
      "- it: 1304\n",
      "- was: 1216\n",
      "\n",
      "10 Least Frequent Words:\n",
      "- ebooks.: 1\n",
      "- newsletter: 1\n",
      "- tosubscribe: 1\n",
      "- gutenbergincluding: 1\n",
      "- includes: 1\n",
      "- www.gutenberg.org.this: 1\n",
      "- searchfacility: 1\n",
      "- pg: 1\n",
      "- paperedition.most: 1\n",
      "- notnecessarily: 1\n",
      "\n",
      "--- (X, y) Pair Generation Example ---\n",
      ". . . . . ---> the\n",
      ". . . . the ---> project\n",
      ". . . the project ---> gutenberg\n",
      ". . the project gutenberg ---> ebook\n",
      ". the project gutenberg ebook ---> of\n",
      "the project gutenberg ebook of ---> the\n",
      "project gutenberg ebook of the ---> adventures\n",
      "gutenberg ebook of the adventures ---> of\n",
      "ebook of the adventures of ---> sherlock\n",
      "of the adventures of sherlock ---> holmesby\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Processing Report for: /content/drive/My Drive/Colab Notebooks/code.txt (Mode: code) ---\n",
      "\n",
      "Vocabulary Size: 32821\n",
      "\n",
      "10 Most Frequent Words:\n",
      "- *: 79576\n",
      "- ): 77515\n",
      "- (: 77390\n",
      "- ;: 71312\n",
      "- ,: 50643\n",
      "- -: 41572\n",
      "- =: 36203\n",
      "- >: 34220\n",
      "- /: 29964\n",
      "- .: 27119\n",
      "\n",
      "10 Least Frequent Words:\n",
      "- rollover: 1\n",
      "- prevails: 1\n",
      "- pid_before: 1\n",
      "- bits_per_page_mask: 1\n",
      "- free_pidmap: 1\n",
      "- proc_pid_init_ino: 1\n",
      "- scales: 1\n",
      "- find_next_offset: 1\n",
      "- mk_pid: 1\n",
      "- pid_hash: 1\n",
      "\n",
      "--- (X, y) Pair Generation Example ---\n",
      ". . . . . ---> /\n",
      ". . . . / ---> *\n",
      ". . . / * ---> *\n",
      ". . / * * ---> linux\n",
      ". / * * linux ---> /\n",
      "/ * * linux / ---> kernel\n",
      "* * linux / kernel ---> /\n",
      "* linux / kernel / ---> irq\n",
      "linux / kernel / irq ---> /\n",
      "/ kernel / irq / ---> autoprobe\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from google.colab import drive\n",
    "\n",
    "# --- 0. Configuration ---\n",
    "path_to_adv_txt = '/content/drive/My Drive/Colab Notebooks/adv.txt'\n",
    "path_to_code_txt = '/content/drive/My Drive/Colab Notebooks/code.txt'\n",
    "\n",
    "# --- 2. Preprocessing Functions ---\n",
    "\n",
    "def preprocess_and_tokenize(text, mode):\n",
    "    \"\"\"\n",
    "    Applies the preprocessing rules based on the dataset type\n",
    "    and returns a list of tokens.\n",
    "    \"\"\"\n",
    "    # Convert the text to lowercase (common for both)\n",
    "    text = text.lower()\n",
    "\n",
    "    if mode == 'text':\n",
    "        text = re.sub(r'[^a-z0-9 \\.]', '', text)\n",
    "        tokens = text.split()\n",
    "\n",
    "    elif mode == 'code':\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def process_and_report(file_path, mode):\n",
    "    \"\"\"\n",
    "    Loads, processes, and prints the full report for a given file.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing Report for: {file_path} (Mode: {mode}) ---\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            corpus = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{file_path}'\")\n",
    "        print(\"Please make sure the file exists and the path is correct.\\n\")\n",
    "        return \n",
    "\n",
    "    # 2. Preprocess\n",
    "    all_tokens = preprocess_and_tokenize(corpus, mode)\n",
    "\n",
    "    if not all_tokens:\n",
    "        print(\"No tokens were found after preprocessing. Is the file empty?\\n\")\n",
    "        return\n",
    "\n",
    "    vocabulary = set(all_tokens)\n",
    "    word_counts = Counter(all_tokens)\n",
    "\n",
    "    # 3. Report Frequencies\n",
    "    print(f\"\\nVocabulary Size: {len(vocabulary)}\")\n",
    "\n",
    "    most_frequent = word_counts.most_common(10)\n",
    "    print(\"\\n10 Most Frequent Words:\")\n",
    "    for word, count in most_frequent:\n",
    "        print(f\"- {word}: {count}\")\n",
    "\n",
    "    least_frequent = word_counts.most_common()[:-11:-1]\n",
    "    print(\"\\n10 Least Frequent Words:\")\n",
    "    for word, count in least_frequent:\n",
    "        print(f\"- {word}: {count}\")\n",
    "\n",
    "    # 4. Report (X, y) Pairs\n",
    "    print(\"\\n--- (X, y) Pair Generation Example ---\")\n",
    "    CONTEXT_SIZE = 5\n",
    "    padding = ['.'] * CONTEXT_SIZE\n",
    "    padded_tokens = padding + all_tokens\n",
    "\n",
    "    training_pairs = []\n",
    "    for i in range(len(padded_tokens) - CONTEXT_SIZE):\n",
    "        context = padded_tokens[i : i + CONTEXT_SIZE]\n",
    "        target = padded_tokens[i + CONTEXT_SIZE]\n",
    "        training_pairs.append((context, target))\n",
    "\n",
    "    for context, target in training_pairs[:10]:\n",
    "        print(f\"{' '.join(context)} ---> {target}\")\n",
    "    print(\"\\n\") \n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "\n",
    "process_and_report(path_to_adv_txt, 'text')\n",
    "\n",
    "print(\"=\"*60 + \"\\n\") \n",
    "\n",
    "process_and_report(path_to_code_txt, 'code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1762244854468,
     "user": {
      "displayName": "ZeD",
      "userId": "14300756461738781152"
     },
     "user_tz": -330
    },
    "id": "SUwXpfNH2nSN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oreDrta-lh8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOC5TMIG/i50Egn45eBDt0T",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
