{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOC5TMIG/i50Egn45eBDt0T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W37QOm-24GoL","executionInfo":{"status":"ok","timestamp":1762243199997,"user_tz":-330,"elapsed":33916,"user":{"displayName":"ZeD","userId":"14300756461738781152"}},"outputId":"2bc9a4e5-271b-49cf-9b34-d430aec6d924"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11lovkNz2Pnp","executionInfo":{"status":"ok","timestamp":1762244244185,"user_tz":-330,"elapsed":4406,"user":{"displayName":"ZeD","userId":"14300756461738781152"}},"outputId":"994c253d-8073-4144-c5bb-e05bc7885970"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Processing Report for: /content/drive/My Drive/Colab Notebooks/adv.txt (Mode: text) ---\n","\n","Vocabulary Size: 16821\n","\n","10 Most Frequent Words:\n","- the: 4795\n","- and: 2572\n","- of: 2458\n","- to: 2435\n","- i: 2420\n","- a: 2344\n","- in: 1607\n","- that: 1494\n","- it: 1304\n","- was: 1216\n","\n","10 Least Frequent Words:\n","- ebooks.: 1\n","- newsletter: 1\n","- tosubscribe: 1\n","- gutenbergincluding: 1\n","- includes: 1\n","- www.gutenberg.org.this: 1\n","- searchfacility: 1\n","- pg: 1\n","- paperedition.most: 1\n","- notnecessarily: 1\n","\n","--- (X, y) Pair Generation Example ---\n",". . . . . ---> the\n",". . . . the ---> project\n",". . . the project ---> gutenberg\n",". . the project gutenberg ---> ebook\n",". the project gutenberg ebook ---> of\n","the project gutenberg ebook of ---> the\n","project gutenberg ebook of the ---> adventures\n","gutenberg ebook of the adventures ---> of\n","ebook of the adventures of ---> sherlock\n","of the adventures of sherlock ---> holmesby\n","\n","\n","============================================================\n","\n","--- Processing Report for: /content/drive/My Drive/Colab Notebooks/code.txt (Mode: code) ---\n","\n","Vocabulary Size: 32821\n","\n","10 Most Frequent Words:\n","- *: 79576\n","- ): 77515\n","- (: 77390\n","- ;: 71312\n","- ,: 50643\n","- -: 41572\n","- =: 36203\n","- >: 34220\n","- /: 29964\n","- .: 27119\n","\n","10 Least Frequent Words:\n","- rollover: 1\n","- prevails: 1\n","- pid_before: 1\n","- bits_per_page_mask: 1\n","- free_pidmap: 1\n","- proc_pid_init_ino: 1\n","- scales: 1\n","- find_next_offset: 1\n","- mk_pid: 1\n","- pid_hash: 1\n","\n","--- (X, y) Pair Generation Example ---\n",". . . . . ---> /\n",". . . . / ---> *\n",". . . / * ---> *\n",". . / * * ---> linux\n",". / * * linux ---> /\n","/ * * linux / ---> kernel\n","* * linux / kernel ---> /\n","* linux / kernel / ---> irq\n","linux / kernel / irq ---> /\n","/ kernel / irq / ---> autoprobe\n","\n","\n"]}],"source":["import re\n","from collections import Counter\n","from google.colab import drive\n","\n","# --- 0. Configuration ---\n","# ⬇️ EDIT THESE TWO PATHS to match your files in Google Drive ⬇️\n","path_to_adv_txt = '/content/drive/My Drive/Colab Notebooks/adv.txt'\n","path_to_code_txt = '/content/drive/My Drive/Colab Notebooks/code.txt'\n","\n","# --- 2. Preprocessing Functions ---\n","\n","def preprocess_and_tokenize(text, mode):\n","    \"\"\"\n","    Applies the preprocessing rules based on the dataset type\n","    and returns a list of tokens.\n","    \"\"\"\n","    # Convert the text to lowercase (common for both)\n","    text = text.lower()\n","\n","    if mode == 'text':\n","        # Rule for text: Remove special characters except alphanumeric, space, and full-stop.\n","        text = re.sub(r'[^a-z0-9 \\.]', '', text)\n","        # Tokenize by splitting on whitespace\n","        tokens = text.split()\n","\n","    elif mode == 'code':\n","        # Rule for code: \"Cannot ignore special characters.\"\n","        # This regex finds all \"words\" (including underscores) OR\n","        # any single special character. This keeps all symbols as tokens.\n","        tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n","\n","    return tokens\n","\n","def process_and_report(file_path, mode):\n","    \"\"\"\n","    Loads, processes, and prints the full report for a given file.\n","    \"\"\"\n","    print(f\"--- Processing Report for: {file_path} (Mode: {mode}) ---\")\n","\n","    # 1. Load Data\n","    try:\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            corpus = f.read()\n","    except FileNotFoundError:\n","        print(f\"Error: File not found at '{file_path}'\")\n","        print(\"Please make sure the file exists and the path is correct.\\n\")\n","        return # Stop processing this file, but don't crash\n","\n","    # 2. Preprocess\n","    all_tokens = preprocess_and_tokenize(corpus, mode)\n","\n","    if not all_tokens:\n","        print(\"No tokens were found after preprocessing. Is the file empty?\\n\")\n","        return\n","\n","    vocabulary = set(all_tokens)\n","    word_counts = Counter(all_tokens)\n","\n","    # 3. Report Frequencies\n","    print(f\"\\nVocabulary Size: {len(vocabulary)}\")\n","\n","    most_frequent = word_counts.most_common(10)\n","    print(\"\\n10 Most Frequent Words:\")\n","    for word, count in most_frequent:\n","        print(f\"- {word}: {count}\")\n","\n","    least_frequent = word_counts.most_common()[:-11:-1]\n","    print(\"\\n10 Least Frequent Words:\")\n","    for word, count in least_frequent:\n","        print(f\"- {word}: {count}\")\n","\n","    # 4. Report (X, y) Pairs\n","    print(\"\\n--- (X, y) Pair Generation Example ---\")\n","    CONTEXT_SIZE = 5\n","    padding = ['.'] * CONTEXT_SIZE\n","    padded_tokens = padding + all_tokens\n","\n","    training_pairs = []\n","    for i in range(len(padded_tokens) - CONTEXT_SIZE):\n","        context = padded_tokens[i : i + CONTEXT_SIZE]\n","        target = padded_tokens[i + CONTEXT_SIZE]\n","        training_pairs.append((context, target))\n","\n","    for context, target in training_pairs[:10]:\n","        print(f\"{' '.join(context)} ---> {target}\")\n","    print(\"\\n\") # Add spacing for the next report\n","\n","\n","# --- Main execution ---\n","# Run processing for both files, one after the other.\n","\n","process_and_report(path_to_adv_txt, 'text')\n","\n","print(\"=\"*60 + \"\\n\") # Add a big separator\n","\n","process_and_report(path_to_code_txt, 'code')"]},{"cell_type":"code","source":[],"metadata":{"id":"SUwXpfNH2nSN","executionInfo":{"status":"ok","timestamp":1762244854468,"user_tz":-330,"elapsed":56,"user":{"displayName":"ZeD","userId":"14300756461738781152"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2oreDrta-lh8"},"execution_count":null,"outputs":[]}]}